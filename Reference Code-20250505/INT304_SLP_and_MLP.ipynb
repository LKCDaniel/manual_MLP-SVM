{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe930dfa",
   "metadata": {},
   "source": [
    "# ðŸ§ª Task: Convert a Single-Layer Perceptron (SLP) into a Multi-Layer Perceptron (MLP)\n",
    "\n",
    "You are given a working implementation of a Single-Layer Perceptron (SLP).  \n",
    "Your goal is to **extend it into a Multi-Layer Perceptron (MLP)** with **one hidden layer**.  \n",
    "You need to modify the following functions:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. `initialize_weights(self)`\n",
    "\n",
    "**SLP:** Initializes a single weight matrix from input to output.  \n",
    "**MLP:** You need to initialize **two weight matrices**:\n",
    "- `W1`: connects **input â†’ hidden layer**\n",
    "- `W2`: connects **hidden layer â†’ output layer**\n",
    "\n",
    "ðŸ“Œ **Hint:** Use small random values.\n",
    "\n",
    "\n",
    "## 2. `forward(self, x, W1, W2, no_gradient=False)`\n",
    "\n",
    "**SLP:** Computes output directly: `a = sigmoid(Wáµ€ Â· x)`  \n",
    "**MLP:** Add hidden layer and activation:\n",
    "\n",
    "ðŸ“Œ **Hint:** If `no_gradient=False`, store intermediate variables for backpropagation:\n",
    "\n",
    "\n",
    "## 3. `back_prop(self, y, W2)`\n",
    "\n",
    "**SLP:** Only computes one gradient.  \n",
    "**MLP:** Use chain rule for backpropagation:\n",
    "\n",
    "ðŸ“Œ **Hint:** Use `self.relu_derivative(z1)` for computing `ReLU'(z1)`.\n",
    "\n",
    "\n",
    "## 4. `update_weights(self, W1, W2, grad_W1, grad_W2, learning_rate)`\n",
    "\n",
    "ðŸ“Œ **Hint:** Update both W1 and W2.\n",
    "\n",
    "\n",
    "## 5. `train(self)`\n",
    "\n",
    "Update the following parts in `train()`:\n",
    "- Replace `initialize_weights()` to return `W1, W2`\n",
    "- Call updated `forward(x, W1, W2, ...)`\n",
    "- Call new `back_prop(y, W2)`\n",
    "- Update both `W1` and `W2` with `update_weights(...)`\n",
    "\n",
    "ðŸ“Œ Evaluate the model using updated `forward()` at each epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b45b15",
   "metadata": {},
   "source": [
    "## Sample Code for SLP:"
   ]
  },
  {
   "cell_type": "code",
   "id": "717e903e",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-05T10:45:51.443761Z",
     "start_time": "2025-05-05T10:45:51.440707Z"
    }
   },
   "source": [
    "# import necessary libs\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# random seed\n",
    "random.seed(int(time.time()))\n",
    "np.random.seed(int(time.time()))"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "69aa9d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T10:45:57.424398Z",
     "start_time": "2025-05-05T10:45:57.416336Z"
    }
   },
   "source": [
    "#define dataloader, You have to put the Iris DataSet at the same path.\n",
    "class Dataloader():\n",
    "    def __init__(self,normalization=False):\n",
    "        self.cross_val_flag = 0\n",
    "        # Read data from iris.data\n",
    "        datalist = self.load_data()\n",
    "        # Shuffle data\n",
    "        self.shuffle_data(datalist)\n",
    "        # Split into 5 folds\n",
    "        self.splited = self.split(datalist, 5)\n",
    "        self.datas, self.labels = self.convert_list(self, self.splited)\n",
    "        if normalization:\n",
    "            self.data_normalization()\n",
    "        self.a = 0\n",
    "\n",
    "    # Cross-Valication here\n",
    "    def get_Train_test_data(self):\n",
    "        temp_data = np.ones((30, 4))\n",
    "        temp_labels = np.ones(30)\n",
    "        test_data = np.ones((30, 4))\n",
    "        test_labels = np.ones(30)\n",
    "        for i in range(5):\n",
    "            if self.cross_val_flag != i:\n",
    "                temp_data = np.vstack((temp_data, self.datas[i]))\n",
    "                temp_labels = np.hstack((temp_labels, self.labels[i]))\n",
    "            else:\n",
    "                test_data = np.vstack((test_data, self.datas[i]))\n",
    "                test_labels = np.hstack((test_labels, self.labels[i]))\n",
    "        self.cross_val_flag += 1 if self.cross_val_flag < 4 else 0\n",
    "        return temp_data[30:, :], temp_labels[30:], test_data[30:, :], test_labels[30:]\n",
    "\n",
    "    # One-hot encoding labels\n",
    "    @staticmethod\n",
    "    def one_hot_mapping(labels: np.ndarray, num_classes: int):\n",
    "        labels = labels.astype(int)\n",
    "        num_labels = len(labels.tolist())\n",
    "        # Generate all zero one-hot Martix\n",
    "        labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "        # Calculate each class's offset position in finally Matrix\n",
    "        index_offset = np.arange(num_labels) * num_classes\n",
    "        # Go through Matrix, put 1 in each position\n",
    "        # Output value setting, may set to 0.99 practical experience\n",
    "        labels_one_hot.flat[index_offset + labels] = 0.99\n",
    "        return labels_one_hot\n",
    "\n",
    "    # Load the data, each row is a object.\n",
    "    @staticmethod\n",
    "    def load_data():\n",
    "        root = \"Iris DataSets/iris.data\"\n",
    "        datalist = []\n",
    "        with open(root, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line == \"\\n\" or line == \"\":\n",
    "                    break\n",
    "                else:\n",
    "                    datalist.append(line.replace(\"\\n\", \"\"))\n",
    "        return datalist\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    @staticmethod\n",
    "    def shuffle_data(datalist: list):\n",
    "        random.shuffle(datalist)\n",
    "        return datalist\n",
    "\n",
    "    # Split into 5 sub-datasets\n",
    "    @staticmethod\n",
    "    def split(datalist: list, splict_fold_num=5):\n",
    "        length = len(datalist)\n",
    "        splited = []\n",
    "        fold_size = length / splict_fold_num\n",
    "        for i in range(splict_fold_num):\n",
    "            temp = []\n",
    "            for j in range(int(i * fold_size), int((i + 1) * fold_size)):\n",
    "                temp.append(datalist[j])\n",
    "            splited.append(temp)\n",
    "        return splited\n",
    "\n",
    "    # Change the str labels to int labels\n",
    "    @staticmethod\n",
    "    def convert_strlabel_to_numlabel(label: str):\n",
    "        # Mapping classes into numbers\n",
    "        num_class_mapping = {\"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\": 2}\n",
    "        for key, value in num_class_mapping.items():\n",
    "            if key == label:\n",
    "                return int(value)\n",
    "\n",
    "    # Modify the data form to List [np0, np1, np2, np3, np4]\n",
    "    # Convert list to ndarray\n",
    "    @staticmethod\n",
    "    def convert_list(self, splited: list):\n",
    "        datas = []\n",
    "        labels = []\n",
    "        for i in range(5):\n",
    "            data = np.empty([30, 4])\n",
    "            label = np.empty([30, ])\n",
    "            for j in range(30):\n",
    "                temp = splited[i][j].split(\",\")\n",
    "                data[j][0], data[j][1] = temp[0], temp[1]\n",
    "                data[j][2], data[j][3] = temp[2], temp[3]\n",
    "                # label convert to num\n",
    "                label[j] = self.convert_strlabel_to_numlabel(temp[4])\n",
    "            datas.append(data)\n",
    "            labels.append(label)\n",
    "        return datas, labels\n",
    "\n",
    "    def data_normalization(self):\n",
    "        x = np.zeros_like(self.datas[0])\n",
    "        for i in range(len(self.datas)):\n",
    "            x = np.vstack((x, self.datas[i]))\n",
    "        x = x[30:, :]\n",
    "        for i in range(4):\n",
    "            # calculate mean and std\n",
    "            mean = np.mean(x[:, i])\n",
    "            std = np.std(x[:, i])\n",
    "            # Do data Normalization\n",
    "            for j in range(len(self.datas)):\n",
    "                self.datas[j][:,i] -=mean\n",
    "                self.datas[j][:,i] /= std\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "944776de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T10:46:02.057554Z",
     "start_time": "2025-05-05T10:46:02.049725Z"
    }
   },
   "source": [
    "#define SLP\n",
    "class SLP():\n",
    "    def __init__(self, dataloader: Dataloader, epoch: int, learning_rate: float,gamma=1,):\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma  # learning_rate decay hyperparameter gamma\n",
    "        self.epoch = epoch\n",
    "\n",
    "        self.weights1_list = []\n",
    "\n",
    "        # Metrics\n",
    "        self.train_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.test_loss = []\n",
    "        self.test_accuracy = []\n",
    "\n",
    "        # Dataloader\n",
    "        self.dataloader = dataloader\n",
    "        self.inter_variable = {}\n",
    "\n",
    "        # Gradient Descent Parameter\n",
    "        self.momentum_v_layer1 = 0\n",
    "        self.momentum_beta = 0.9\n",
    "\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        w1 = np.random.rand(5, 3) * 2 - 1\n",
    "        return w1\n",
    "\n",
    "    def forward(self, x, w1, no_gradient: bool):\n",
    "        z1 = w1.T.dot(x)\n",
    "        a1 = 1 / (1 + np.exp(-z1))\n",
    "        if no_gradient:\n",
    "            # for predict\n",
    "            return a1\n",
    "        else:\n",
    "            # For back propagation\n",
    "            self.inter_variable = {\"z1\": z1, \"a1\": a1}\n",
    "            return a1\n",
    "\n",
    "    def back_prop(self, x, y):\n",
    "        delta_k = self.inter_variable[\"a1\"] - y.T\n",
    "        delta_j = self.inter_variable[\"a1\"] * \\\n",
    "                    (1 - self.inter_variable[\"a1\"]) * delta_k\n",
    "        gradient1 = x.dot(delta_j.T)\n",
    "        return gradient1\n",
    "\n",
    "    def update_weight(self, w1, gradient1, learning_rate):\n",
    "        w1 -= learning_rate * gradient1\n",
    "        return w1, learning_rate\n",
    "    \n",
    "    def update_learning_rate(self, learning_rate):\n",
    "        # Learning rate decay\n",
    "        learning_rate *= self.gamma\n",
    "        return learning_rate\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(label, y_hat: np.ndarray):\n",
    "        y_hat = y_hat.T\n",
    "        acc = y_hat.argmax(axis=1) == label.argmax(axis=1)\n",
    "        b = acc + 0\n",
    "        return b.mean()\n",
    "\n",
    "    def save(self, filename):\n",
    "        np.savez(filename, self.weights1_list)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(output, label):\n",
    "        return np.sum(((output.T - label) ** 2)) / (2 * label.shape[0])\n",
    "\n",
    "    def train(self):\n",
    "        start = time.time()\n",
    "        # Do 5 fold corss validation\n",
    "        for i in range(5):\n",
    "            with tqdm(total= self.epoch) as _tqdm:\n",
    "                _tqdm.set_description('5_Fold: {}/{}'.format(i + 1, 5))\n",
    "\n",
    "                # Get data, everytime call this function, do cross validation load data\n",
    "                learning_rate = self.learning_rate\n",
    "                train_data, train_labels, test_data, test_labels = self.dataloader.get_Train_test_data()\n",
    "\n",
    "                # Add one dimension for b\n",
    "                train_data = np.c_[train_data, np.ones(train_data.shape[0])].T\n",
    "                test_data = np.c_[test_data, np.ones(test_data.shape[0])].T\n",
    "\n",
    "                train_labels = self.dataloader.one_hot_mapping(train_labels, 3)\n",
    "                test_labels = self.dataloader.one_hot_mapping(test_labels, 3)\n",
    "\n",
    "                # initialize weights\n",
    "                w1 = self.initialize_weights()\n",
    "\n",
    "                # put each cross validation metrics here\n",
    "                temp_train_accuracy = []\n",
    "                temp_train_loss = []\n",
    "                temp_test_accuracy = []\n",
    "                temp_test_loss = []\n",
    "                for j in range(self.epoch):\n",
    "                    for k in range(train_data.shape[1]):\n",
    "\n",
    "                        single_data = train_data[:, k].reshape((5, 1))\n",
    "                        single_label = train_labels[k].reshape(1, 3)\n",
    "\n",
    "                        # forward feed\n",
    "                        output = self.forward(x=single_data, w1=w1, no_gradient=False)\n",
    "\n",
    "                        # Calculate gradient\n",
    "                        gradient1 = self.back_prop(x=single_data, y=single_label)\n",
    "\n",
    "                        # Evaluate\n",
    "                        train_accuracy = self.accuracy(single_label, output)\n",
    "                        train_loss = self.loss(output, single_label)\n",
    "                        test_forward = self.forward(test_data, w1, no_gradient=True)\n",
    "                        test_accuracy = self.accuracy(test_labels, test_forward)\n",
    "                        test_loss = self.loss(test_forward, test_labels)\n",
    "\n",
    "                        # Data storage\n",
    "                        temp_train_accuracy.append(train_accuracy)\n",
    "                        temp_train_loss.append(train_loss)\n",
    "                        temp_test_accuracy.append(test_accuracy)\n",
    "                        temp_test_loss.append(test_loss)\n",
    "\n",
    "                        # Update weights\n",
    "                        w1, learning_rate = self.update_weight(w1, gradient1, learning_rate=learning_rate)\n",
    "                    # Update learning rate every epoch\n",
    "                    learning_rate = self.update_learning_rate(learning_rate)\n",
    "                    _tqdm.set_postfix(learning_rate='{:.6f}'.format(learning_rate), train_loss='{:.6f}'.format(train_loss), train_accuracy='{:.6f}'.format(train_accuracy), test_loss='{:.6f}'.format(test_loss), test_accuracy='{:.6f}'.format(test_accuracy))\n",
    "                    _tqdm.update(1)\n",
    "\n",
    "\n",
    "            self.weights1_list.append(w1)\n",
    "            self.train_accuracy.append(temp_train_accuracy)\n",
    "            self.train_loss.append(temp_train_loss)\n",
    "            self.test_accuracy.append(temp_test_accuracy)\n",
    "            self.test_loss.append(temp_test_loss)\n",
    "\n",
    "        end = time.time()\n",
    "        sum = 0\n",
    "        for i in range(len(self.test_accuracy)):\n",
    "            sum += self.test_accuracy[i][-1]\n",
    "        print(f\"Average test accuracy:{sum / 5}\")\n",
    "        print(f\"Trained time: {1000 * (end - start)} ms\")"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "e9fe82ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T10:50:33.100541Z",
     "start_time": "2025-05-05T10:50:33.069706Z"
    }
   },
   "source": [
    "# Run SLP\n",
    "# normalization:\"True\", \"False\"\n",
    "dataloader = Dataloader(normalization=True)\n",
    "# gamma is learning rate decay hyperparameter\n",
    "SLP = SLP(dataloader=dataloader, epoch=2000, learning_rate=0.001, gamma=0.9998)\n",
    "# training\n",
    "SLP.train()"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Iris DataSets/iris.data'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Run SLP\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# normalization:\"True\", \"False\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m dataloader = \u001B[43mDataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnormalization\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# gamma is learning rate decay hyperparameter\u001B[39;00m\n\u001B[32m      5\u001B[39m SLP = SLP(dataloader=dataloader, epoch=\u001B[32m2000\u001B[39m, learning_rate=\u001B[32m0.001\u001B[39m, gamma=\u001B[32m0.9998\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 6\u001B[39m, in \u001B[36mDataloader.__init__\u001B[39m\u001B[34m(self, normalization)\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28mself\u001B[39m.cross_val_flag = \u001B[32m0\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# Read data from iris.data\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m datalist = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Shuffle data\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;28mself\u001B[39m.shuffle_data(datalist)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 51\u001B[39m, in \u001B[36mDataloader.load_data\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     49\u001B[39m root = \u001B[33m\"\u001B[39m\u001B[33mIris DataSets/iris.data\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     50\u001B[39m datalist = []\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[32m     52\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m f:\n\u001B[32m     53\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m line == \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m line == \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Program Files\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:326\u001B[39m, in \u001B[36m_modified_open\u001B[39m\u001B[34m(file, *args, **kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m}:\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    321\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIPython won\u001B[39m\u001B[33m'\u001B[39m\u001B[33mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m by default \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    322\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    323\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33myou can use builtins\u001B[39m\u001B[33m'\u001B[39m\u001B[33m open.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    324\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'Iris DataSets/iris.data'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "d1fd9cf4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06ecd5",
   "metadata": {},
   "source": [
    "## MLP code for filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fb5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, dataloader, epoch: int, learning_rate: float, gamma=1, hidden_dim=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epoch = epoch\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.train_loss = []\n",
    "        self.train_accuracy = []\n",
    "        self.test_loss = []\n",
    "        self.test_accuracy = []\n",
    "\n",
    "        self.dataloader = dataloader\n",
    "        self.inter_variable = {}\n",
    "        self.weights1_list = []\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        ________  # TODO: Initialize two sets of weights: W1 for inputâ†’hidden, W2 for hiddenâ†’output\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    def forward(self, x, W1, W2, no_gradient=False):\n",
    "        ________  # TODO: Add one hidden layer between input and output. Save intermediate variables.\n",
    "\n",
    "    def back_prop(self, y, W2):\n",
    "        ________  # TODO: Backpropagate through two layers: output and hidden. Use chain rule.\n",
    "\n",
    "    def update_weights(self, W1, W2, grad_W1, grad_W2, learning_rate):\n",
    "        ________  # TODO: Update both sets of weights using gradients\n",
    "\n",
    "    def update_learning_rate(self, learning_rate):\n",
    "        return learning_rate * self.gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(label, y_hat):\n",
    "        y_hat = y_hat.T\n",
    "        return (y_hat.argmax(axis=1) == label.argmax(axis=1)).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(output, label):\n",
    "        return np.sum((output.T - label) ** 2) / (2 * label.shape[0])\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(5):\n",
    "            learning_rate = self.learning_rate\n",
    "            train_data, train_labels, test_data, test_labels = self.dataloader.get_Train_test_data()\n",
    "\n",
    "            train_data = np.c_[train_data, np.ones(train_data.shape[0])].T\n",
    "            test_data = np.c_[test_data, np.ones(test_data.shape[0])].T\n",
    "\n",
    "            train_labels = self.dataloader.one_hot_mapping(train_labels, 3)\n",
    "            test_labels = self.dataloader.one_hot_mapping(test_labels, 3)\n",
    "\n",
    "            W1, W2 = self.initialize_weights()\n",
    "\n",
    "            temp_train_accuracy, temp_train_loss = [], []\n",
    "            temp_test_accuracy, temp_test_loss = [], []\n",
    "\n",
    "            for j in tqdm(range(self.epoch), desc=f'Fold {i+1}/5'):\n",
    "                for k in range(train_data.shape[1]):\n",
    "                    single_data = train_data[:, k].reshape((5, 1))\n",
    "                    single_label = train_labels[k].reshape(1, 3)\n",
    "\n",
    "                    ________  # TODO: Run forward pass using W1 and W2\n",
    "\n",
    "                    ________  # TODO: Compute gradients using backpropagation through two layers\n",
    "\n",
    "                    W1, W2 = self.update_weights(W1, W2, grad_W1, grad_W2, learning_rate)\n",
    "\n",
    "                test_output = self.forward(test_data, W1, W2, no_gradient=True)\n",
    "                test_accuracy = self.accuracy(test_labels, test_output)\n",
    "                test_loss = self.loss(test_output, test_labels)\n",
    "\n",
    "                temp_test_accuracy.append(test_accuracy)\n",
    "                temp_test_loss.append(test_loss)\n",
    "\n",
    "                learning_rate = self.update_learning_rate(learning_rate)\n",
    "\n",
    "            self.weights1_list.append((W1, W2))\n",
    "            self.train_accuracy.append(temp_train_accuracy)\n",
    "            self.train_loss.append(temp_train_loss)\n",
    "            self.test_accuracy.append(temp_test_accuracy)\n",
    "            self.test_loss.append(temp_test_loss)\n",
    "\n",
    "        print(f\"Average test accuracy: {np.mean([acc[-1] for acc in self.test_accuracy])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SLP\n",
    "# normalization:\"True\", \"False\"\n",
    "dataloader = Dataloader(normalization=True)\n",
    "# gamma is learning rate decay hyperparameter\n",
    "MLP = MLP(dataloader=dataloader, epoch=2000, learning_rate=0.001, gamma=0.9998, hidden_dim=__) #tune different hidden unit!\n",
    "# training\n",
    "MLP.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e5ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
