{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4423c7c7",
   "metadata": {},
   "source": [
    "# üß± INT304 Support Vector Machine (SVM)\n",
    "\n",
    "The **Support Vector Machine (SVM)** is a powerful supervised learning model that finds the optimal hyperplane separating data from different classes with the **maximum margin**.\n",
    "\n",
    "SVM works well in high-dimensional spaces and is effective when the number of features exceeds the number of samples.\n",
    "\n",
    "#### üìê Objective Function\n",
    "\n",
    "For linearly separable data, SVM solves:\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\quad \\frac{1}{2} \\|w\\|^2 \\quad \\text{subject to } \\quad y_i(w^T x_i + b) \\geq 1\n",
    "$$\n",
    "\n",
    "This maximizes the margin between classes while ensuring correct classification.\n",
    "\n",
    "#### üß† Kernels\n",
    "\n",
    "To handle non-linearly separable data, SVM uses **kernel functions** to map input data into a higher-dimensional space. Common kernels:\n",
    "\n",
    "- `'linear'`: $K(x_i, x_j) = x_i^T x_j$\n",
    "- `'poly'`: $K(x_i, x_j) = (\\gamma x_i^T x_j + r)^d$\n",
    "- `'rbf'`: $K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)$\n",
    "- `'sigmoid'`: $K(x_i, x_j) = \\tanh(\\gamma x_i^T x_j + r)$\n",
    "\n",
    "We will use `sklearn`'s `SVC` (Support Vector Classifier) to implement SVM efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10250658",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8b045f",
   "metadata": {},
   "source": [
    "## sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b9e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libs\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d4d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Show first few samples\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(digits.images[i], cmap='gray')\n",
    "    ax.set_title(f\"Label: {digits.target[i]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=42)\n",
    "\n",
    "# Train SVM\n",
    "clf = svm.SVC(kernel='linear')  # You may want to tune kernel, C, gamma\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6197671c",
   "metadata": {},
   "source": [
    "### üîß Parameters you may tune for training\n",
    "\n",
    "\n",
    "\n",
    "- ####**`C`: float, default = 1.0**  \n",
    "  Regularization parameter. The strength of the regularization is inversely proportional to $C$.  \n",
    "  Must be strictly positive. The penalty is a squared $L_2$ penalty.\n",
    "\n",
    "- **`kernel`: `'linear'`, `'poly'`, `'rbf'`, `'sigmoid'`, `'precomputed'` or callable, default = `'rbf'`**  \n",
    "  Specifies the kernel type to be used in the algorithm.  \n",
    "  If none is given, `'rbf'` will be used.  \n",
    "  If a callable is given, it is used to pre-compute the kernel matrix from data matrices;  \n",
    "  that matrix should be an array of shape $(n\\_samples, n\\_samples)$.  \n",
    "  For an intuitive visualization of different kernel types, see *Plot classification boundaries with different SVM Kernels*.\n",
    "\n",
    "- **`degree`: int, default = 3**  \n",
    "  Degree of the polynomial kernel function (`'poly'`). Must be non-negative.  \n",
    "  Ignored by all other kernels.\n",
    "\n",
    "- **`coef0`: float, default = 0.0**  \n",
    "  Independent term in kernel function. It is only significant in `'poly'` and `'sigmoid'`.\n",
    "\n",
    "- **`tol`: float, default = $1e-3$**  \n",
    "  Tolerance for stopping criterion.\n",
    "  \n",
    "#### Observe:\n",
    "\n",
    "- Does overfitting or underfitting occur with certain parameter choices?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f48f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ce72d",
   "metadata": {},
   "source": [
    "## `sklearn.svm.SVR`\n",
    "\n",
    "**Support Vector Regression (SVR)** is an extension of Support Vector Machines (SVM) to regression tasks. Unlike linear regression, which minimizes squared errors, SVR aims to fit a function that deviates from actual targets by at most $\\epsilon$, while also being as flat as possible.\n",
    "\n",
    "#### üßÆ Optimization Objective\n",
    "\n",
    "SVR minimizes the following:\n",
    "\n",
    "$$\n",
    "\\min_{w, b, \\xi, \\xi^*} \\ \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^*)\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_i - (w^T x_i + b) &\\leq \\epsilon + \\xi_i \\\\\n",
    "(w^T x_i + b) - y_i &\\leq \\epsilon + \\xi_i^* \\\\\n",
    "\\xi_i, \\xi_i^* &\\geq 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $C$ is the regularization parameter\n",
    "- $\\epsilon$ defines a margin of tolerance (called the $\\epsilon$-tube)\n",
    "- $\\xi_i$, $\\xi_i^*$ are slack variables for errors outside the $\\epsilon$-tube\n",
    "\n",
    "SVR works well when the data is noisy and you want to balance between **model complexity** and **precision tolerance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ddb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "Y = raw_df.values[1::2, 2]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "norm_x_train = scaler.fit_transform(x_train)\n",
    "norm_x_test = scaler.transform(x_test)\n",
    "\n",
    "model = SVR() # you need tuning here\n",
    "model.fit(norm_x_train, y_train)\n",
    "y_pred = model.predict(norm_x_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'Test MSE: {test_mse}, Test MAE: {test_mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff863f91",
   "metadata": {},
   "source": [
    "Plot actual vs predicted values. Are there consistent under- or over-estimations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e369cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot actual vs predicted values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"SVR Prediction vs Actual\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89893cc",
   "metadata": {},
   "source": [
    "### üîß Parameters you may tune for training\n",
    "\n",
    "\n",
    "\n",
    "- **`kernel`: `'linear'`, `'poly'`, `'rbf'`, `'sigmoid'`, `'precomputed'` or callable, default = `'rbf'`**  \n",
    "  Specifies the kernel type to be used in the algorithm.  \n",
    "  If none is given, `'rbf'` will be used.  \n",
    "  If a callable is given, it is used to precompute the kernel matrix.\n",
    "\n",
    "- **`degree`: int, default = 3**  \n",
    "  Degree of the polynomial kernel function (`'poly'`). Must be non-negative.  \n",
    "  Ignored by all other kernels.\n",
    "\n",
    "- **`coef0`: float, default = 0.0**  \n",
    "  Independent term in kernel function. It is only significant in `'poly'` and `'sigmoid'`.\n",
    "\n",
    "- **`tol`: float, default = $1e-3$**  \n",
    "  Tolerance for stopping criterion.\n",
    "\n",
    "- **`C`: float, default = 1.0**  \n",
    "  Regularization parameter. The strength of the regularization is inversely proportional to $C$.  \n",
    "  Must be strictly positive. The penalty is a squared $L_2$ penalty.\n",
    "\n",
    "- **`epsilon`: float, default = 0.1**  \n",
    "  Epsilon in the epsilon-SVR model.  \n",
    "  It specifies the epsilon-tube within which no penalty is associated in the training loss function  \n",
    "  with points predicted within a distance epsilon from the actual value.  \n",
    "  Must be non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8231bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
